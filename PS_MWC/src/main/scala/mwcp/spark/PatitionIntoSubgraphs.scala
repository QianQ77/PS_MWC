package mwcp.spark

import mwcp.spark.MWCP._
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.graphx.{VertexId, _}
import org.apache.spark.rdd.RDD
import org.apache.log4j._

import scala.collection.mutable.{ArrayBuffer, Buffer}
import scala.collection.Map
import scala.Array

/**
  * Created by qiuqian on 3/22/18.
  */
object PatitionIntoSubgraphs extends LogHelper{

  def partition(weightedGraph: Graph[Double, Int], optimumMwcl: MWCL, sc: SparkContext) : List[Subgraph]= {

    val startTime = System.currentTimeMillis()

    weightedGraph.cache()

    val degreeGraph = weightedGraph.outerJoinVertices(weightedGraph.degrees)((vid, weight, degOpt) => (weight, degOpt.getOrElse(0)))

    logInfo("generated degreeGraph")
    //send message from higher-degree vertex to lower-degree vertex
    // so that each vertex can get all its higher-degree neighbors

    //To improve performance the primary aggregation operator changed from graph.mapReduceTriplets to the new graph.AggregateMessages.

    /*
    --without pruning---
    val nbVertices: VertexRDD[Set[VertexId]] = {
      degreeGraph.aggregateMessages[Set[VertexId]] (
        triplet => {

          if(triplet.srcAttr._2 > triplet.dstAttr._2) {
            //if src has higher degree, send src's id to dest
            triplet.sendToDst(Set(triplet.srcId))
          }else if(triplet.srcAttr._2 == triplet.dstAttr._2){
            // if src and dst have equal degree, find send higher id to lower id vertex
            if(triplet.srcId > triplet.dstId) triplet.sendToDst(Set(triplet.srcId))
            else triplet.sendToSrc(Set(triplet.dstId))
          }else {
            // if desc has higher degree, send desc's id to src
            triplet.sendToSrc(Set(triplet.dstId))
          }
        },
        //merge neighbors
        (n1, n2) => n1 ++ n2
      )
    }
    */


    degreeGraph.cache()

    //take vertices by degree in asc ordering
    //nbVertices the property contains (SetOfNeighbors, SumOfNeighborsWeight)

    val nbVertices: VertexRDD[(Set[VertexId], Double)] = {
      degreeGraph.aggregateMessages[(Set[VertexId], Double)] (
        triplet => {
          //srcAttr and dstAttr: (weight, degree)
          if(triplet.srcAttr._2 > triplet.dstAttr._2) {
            //if src has higher degree, send src's id and weight to dest
            triplet.sendToDst((Set(triplet.srcId), triplet.srcAttr._1))
          }else if(triplet.srcAttr._2 == triplet.dstAttr._2 && triplet.srcId > triplet.dstId){
            // if src and dst have equal degree, send higher id and weight to lower id vertex
            triplet.sendToDst((Set(triplet.srcId), triplet.srcAttr._1))
          }else {
            // if desc has higher degree, send desc's id and weight to src
            triplet.sendToSrc((Set(triplet.dstId), triplet.dstAttr._1))
          }
        },
        //merge neighbors and add up the weights
        (n1, n2) => (n1._1 ++ n2._1, n1._2 + n2._2)
      )
    }

    /*

    //take vertices by degree in desc ordering
    //nbVertices the property contains (SetOfNeighbors, SumOfNeighborsWeight)
    val nbVertices: VertexRDD[(Set[VertexId], Double)] = {
      degreeGraph.aggregateMessages[(Set[VertexId], Double)] (
        triplet => {
          //srcAttr and dstAttr: (weight, degree)
          if(triplet.srcAttr._2 < triplet.dstAttr._2) {
            //if src has lower degree, send src's id and weight to dest
            triplet.sendToDst((Set(triplet.srcId), triplet.srcAttr._1))
          }else if(triplet.srcAttr._2 == triplet.dstAttr._2){
            // if src and dst have equal degree, send lower id and weight to higher id vertex
            if(triplet.srcId < triplet.dstId) triplet.sendToDst((Set(triplet.srcId), triplet.srcAttr._1))
            else triplet.sendToSrc((Set(triplet.dstId), triplet.dstAttr._1))
          }else {
            // if desc has lower degree, send desc's id and weight to src
            triplet.sendToSrc((Set(triplet.dstId), triplet.dstAttr._1))
          }
        },
        //merge neighbors and add up the weights
        (n1, n2) => (n1._1 ++ n2._1, n1._2 + n2._2)
      )
    }
    */

    logInfo("generated nbVertices")

    degreeGraph.unpersist()

    nbVertices.cache()

    var verticesWithNbRDD: VertexRDD[(Double, Set[VertexId])] =
      nbVertices.leftJoin(weightedGraph.vertices)((vid, setAndSumNb, weight) => weight match {
        case Some(weight) =>
          //if sum of this vertex'weight and its higher neighbors' weight is not greater than current
          // clique weight, we don't need to process the subgraph which is generated by this vertex
          if(weight + setAndSumNb._2 <= optimumMwcl.weight) (0D, Set[VertexId]())
          else (weight.toDouble, setAndSumNb._1)
        case None => (0D, Set[VertexId]())
      }).filter(_._2._1 != 0)

    nbVertices.unpersist()
    logInfo("generated verticesWithNbRDD")

    verticesWithNbRDD.cache()

    var subgraphs: Buffer[Subgraph] = Buffer[Subgraph]()
    var propertyMap: Map[VertexId, (Double, Set[VertexId])] = verticesWithNbRDD.collectAsMap()
    var propertyMap_bc: Broadcast[Map[VertexId, (Double, Set[VertexId])]] =
      sc.broadcast(propertyMap)


    var verticesWithNbRDDCount = verticesWithNbRDD.count()
    var currentid = 1
    logInfo("currentid: " + currentid)

    while(currentid < verticesWithNbRDDCount - 1000) {
      var currentProperty: Option[(Double, Set[VertexId])] = propertyMap.get(currentid)
      logInfo("currentProperty: " + currentProperty)

      currentProperty match {
        case Some(property) => subgraphs.append(Subgraph(Option(List((currentid, property._1))),
          weightedGraph.subgraph(vpred = (n_vid, n_weight) => property._2 contains n_vid)))
          logInfo("currentid: " + currentid + ", graph vertices' size: " + subgraphs.last.graph.vertices.count())
        case None => Nil
      }
      currentid = currentid + 1
    }

    verticesWithNbRDD.unpersist()

    subgraphs.append(Subgraph(None, weightedGraph.subgraph(vpred = (vid, weight) => vid >= currentid)))



    var timeCost = (System.currentTimeMillis() - startTime) / 1000
    logInfo("Time Cost: " + timeCost + " seconds")

    //sc.parallelize(subgraphs).cache()
    subgraphs.toList

    /*---------deprecated----------
  var graphWithNb: Graph[(Double, Set[VertexId]), Int] = weightedGraph.outerJoinVertices(nbVertices)((vid, weight, setOpt) =>
   setOpt match {
     case Some(set) => (weight, set.toSet)
     case None => (weight, Set())
   })

 var subgraphs: Buffer[Subgraph] = Buffer[Subgraph]()
 var propertyMap: Map[VertexId, (Double, Set[VertexId])] = graphWithNb.vertices.collectAsMap()

 var currentid = 0
 while(graphWithNb.vertices.count() > currentid + 3) {
   var currentProperty: Option[(Double, Set[VertexId])] = propertyMap.get(currentid)
   currentProperty match {
     case Some(property) => subgraphs.append(Subgraph(Option(List((currentid, property._1))),
       weightedGraph.subgraph(vpred = (n_vid, n_weight) => property._2 contains n_vid)))
     case None => Nil
   }
   currentid = currentid + 1
 }
 subgraphs.append(Subgraph(None, weightedGraph.subgraph(vpred = (vid, weight) => vid > currentid)))
 ---------deprecated----------
 */

    /**
    graphWithNb.mapVertices{case (vid, (weight, set)) => graphWithNb.subgraph(vpred = (n_vid, n_weight) => set contains n_vid)}

      weightedGraph.mapVertices((id, weight) => weightedGraph.subgraph(vpred = (vid, vweight) => vid > id)).vertices.collect

      weightedGraph.mapVertices((id: VertexId, weight: Double) =>
      Subgraph(Option(List((id, weight))),
        weightedGraph.subgraph(vpred =
          (vid, vweight) => propertyMap.get(id) match {
            case None => false
            case Some(property) => property._2 contains vid
          })))


    weightedGraph.mapVertices((id: VertexId, weight: Double) =>
      FastWclq.fastWClq(Subgraph(Option(List((id, weight))),
        weightedGraph.subgraph(vpred =
          (vid, vweight) => propertyMap.get(id) match {
            case None => false
            case Some(property) => property._2 contains vid
          })), 30, optimumMwcl)).vertices

    doesn't work, because Spark doesn't support nesting of RDDs
    (see https://stackoverflow.com/a/14130534/590203 for another occurrence of the same problem),
    so you can't perform transformations or actions on RDDs inside of other RDD operations.
      */



    /* doesn't work,
    var result: Option[MWCL] = nbVertices.filter(!_._2._2.isEmpty).map{case (vid: VertexId, (weight: Double, set)) =>
      new Subgraph(Option(List((vid, weight))),
      weightedGraph.subgraph(vpred = (n_vid, n_weight) => set contains n_vid))}.
      map(fastWClq(_, 30, mwcl)).
      reduce(maxMWCL(_, _))
    */
  }

}
